#!/usr/bin/env python3
"""
çˆ¬è™«åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯• Cookieè·å–ã€ä»£ç†è·å–å’Œé›ªçƒæ•°æ®æŠ“å–åŠŸèƒ½
"""
import asyncio
import json
import sys
import os
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/cenwei/workspace/saturn_mousehunter/saturn-mousehunter-crawler-service/src')

from application.services.xueqiu_core_engine import XueqiuCoreEngine
from infrastructure.settings.config import CrawlerSettings


async def test_cookie_acquisition():
    """æµ‹è¯•Cookieè·å–åŠŸèƒ½"""
    print("\nğŸª æµ‹è¯•Cookieè·å–åŠŸèƒ½...")

    try:
        # 1. å…ˆä»Cookieæ± APIè·å–çœŸå®Cookie
        print("ğŸ“¡ ä»Cookieæ± APIè·å–é›ªçƒCookie...")
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://192.168.8.168:8000/api/v1/md/cookie/request",
                json={"website": "xueqiu.com", "usage_type": "crawling"}
            )

            if response.status_code == 200:
                cookie_response = response.json()
                cookie_id = cookie_response["cookie_id"]
                cookie_data = cookie_response["cookie_data"]

                # æ„å»ºé›ªçƒcookieå­—ç¬¦ä¸²
                cookie_parts = []
                for key, value in cookie_data.items():
                    cookie_parts.append(f"{key}={value}")
                cookie_string = "; ".join(cookie_parts)

                print(f"âœ… ä»Cookieæ± è·å–æˆåŠŸ [æ± ID: {cookie_id[:8]}...]")
                print(f"   Cookieå†…å®¹: {cookie_string[:50]}...")
                return cookie_id, cookie_string
            else:
                print(f"âš ï¸  Cookieæ± APIè°ƒç”¨å¤±è´¥: {response.status_code}")

        # 2. å¦‚æœCookieæ± å¤±è´¥ï¼Œå°è¯•Dragonflyè·å–
        settings = CrawlerSettings()
        engine = XueqiuCoreEngine(settings)
        await engine.initialize()

        test_cookie_ids = ["xueqiu_session_001", "xueqiu_session", "default"]

        for cookie_id in test_cookie_ids:
            cookie = await engine._get_cookie(cookie_id)
            if cookie:
                print(f"âœ… Dragonflyè·å–æˆåŠŸ [{cookie_id}]: {cookie[:50]}...")
                return cookie_id, cookie
            else:
                print(f"âŒ Dragonflyè·å–å¤±è´¥ [{cookie_id}]: None")

        # 3. å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°Cookieï¼Œä½¿ç”¨æ¨¡æ‹ŸCookie
        print("âš ï¸  æœªæ‰¾åˆ°ç°æœ‰Cookieï¼Œä½¿ç”¨æ¨¡æ‹ŸCookieè¿›è¡Œæµ‹è¯•")
        test_cookie = "xq_a_token=test123; xq_r_token=test456; xq_id_token=test789"
        return "test_mock", test_cookie

    except Exception as e:
        print(f"âŒ Cookieè·å–æµ‹è¯•å¤±è´¥: {str(e)}")
        return None, None


async def test_proxy_acquisition():
    """æµ‹è¯•ä»£ç†è·å–åŠŸèƒ½"""
    print("\nğŸŒ æµ‹è¯•ä»£ç†è·å–åŠŸèƒ½...")

    try:
        settings = CrawlerSettings()
        engine = XueqiuCoreEngine(settings)
        await engine.initialize()

        # æµ‹è¯•è·å–ä»£ç†
        proxy = await engine._get_random_proxy()

        if proxy:
            print(f"âœ… ä»£ç†è·å–æˆåŠŸ: {proxy}")
            return proxy
        else:
            print("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼")
            return None

    except Exception as e:
        print(f"âŒ ä»£ç†è·å–æµ‹è¯•å¤±è´¥: {str(e)}")
        return None


async def test_direct_http_crawling(symbol: str, cookie: str, proxy: str = None):
    """æµ‹è¯•ç›´æ¥HTTPæŠ“å–åŠŸèƒ½ï¼ˆä¸ä¾èµ–Dragonflyï¼‰"""
    print(f"\nğŸ“Š æµ‹è¯•ç›´æ¥æŠ“å–è‚¡ç¥¨æ•°æ® [{symbol}]...")

    try:
        import httpx
        import time

        # æ„å»ºé›ªçƒAPIè¯·æ±‚
        url = "https://stock.xueqiu.com/v5/stock/chart/kline.json"

        params = {
            "symbol": symbol,
            "begin": int(time.time() * 1000),  # å½“å‰æ—¶é—´æˆ³
            "period": "day",
            "type": "before",
            "count": -5,  # æœ€è¿‘5æ¡æ•°æ®
            "indicator": "kline,pe,pb,ps,pcf,market_capital"
        }

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": f"https://xueqiu.com/S/{symbol}",
            "Origin": "https://xueqiu.com",
            "X-Requested-With": "XMLHttpRequest",
            "Cookie": cookie
        }

        # é…ç½®ä»£ç†
        proxy_config = None
        if proxy:
            proxy_config = proxy
            print(f"ğŸ”„ ä½¿ç”¨ä»£ç†: {proxy}")
        else:
            print("ğŸ”„ ä½¿ç”¨ç›´è¿æ¨¡å¼")

        timeout = httpx.Timeout(
            connect=10.0,
            read=30.0,
            write=10.0,
            pool=5.0
        )

        async with httpx.AsyncClient(
            timeout=timeout,
            proxy=proxy_config,
            follow_redirects=True
        ) as client:
            response = await client.get(url, params=params, headers=headers)

            print(f"ğŸ“ˆ HTTPå“åº”çŠ¶æ€: {response.status_code}")

            if response.status_code == 200:
                data = response.json()

                if data.get("error_code") == 0:
                    kline_data = data.get("data", {})
                    items = kline_data.get("item", [])

                    print(f"âœ… æ•°æ®æŠ“å–æˆåŠŸ!")
                    print(f"   è‚¡ç¥¨ä»£ç : {symbol}")
                    print(f"   æ•°æ®æ¡æ•°: {len(items)}")
                    print(f"   å“åº”æ—¶é—´: {response.elapsed.total_seconds():.2f}ç§’")

                    # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®ç»“æ„
                    if items:
                        print(f"   æœ€æ–°æ•°æ®ç‚¹: {items[0][:6] if len(items[0]) >= 6 else items[0]}")
                        print(f"   æ•°æ®å­—æ®µ: timestamp, open, high, low, close, volume...")

                    # è¿”å›ç»“æ„åŒ–ç»“æœ
                    return {
                        "success": True,
                        "symbol": symbol,
                        "records_count": len(items),
                        "response_time": response.elapsed.total_seconds(),
                        "status_code": response.status_code,
                        "sample_data": items[:2] if items else [],
                        "full_response": data
                    }
                else:
                    error_msg = data.get("error_description", "æœªçŸ¥é”™è¯¯")
                    print(f"âŒ é›ªçƒAPIè¿”å›é”™è¯¯: {error_msg}")
                    return {
                        "success": False,
                        "error": f"api_error: {error_msg}",
                        "status_code": response.status_code,
                        "response_data": data
                    }
            else:
                print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
                return {
                    "success": False,
                    "error": f"http_error: {response.status_code}",
                    "status_code": response.status_code,
                    "response_text": response.text[:500]
                }

    except Exception as e:
        print(f"âŒ ç›´æ¥æŠ“å–æµ‹è¯•å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": f"exception: {str(e)}"
        }


async def test_xueqiu_core_engine(symbol: str, cookie_id: str, cookie: str, proxy: str = None):
    """æµ‹è¯•é›ªçƒæ ¸å¿ƒå¼•æ“åŠŸèƒ½"""
    print(f"\nğŸš€ æµ‹è¯•é›ªçƒæ ¸å¿ƒå¼•æ“ [{symbol}]...")

    try:
        settings = CrawlerSettings()
        engine = XueqiuCoreEngine(settings)
        await engine.initialize()

        # æ„å»ºä»»åŠ¡æ•°æ®
        task_data = {
            "task_id": f"test_{symbol}_{int(time.time())}",
            "endpoint": "kline",
            "symbol": symbol,
            "cookie_id": cookie_id,
            "proxy": proxy,
            "params": {
                "symbol": symbol,
                "period": "day",
                "count": -5,
                "type": "before"
            }
        }

        # ç”±äºæ²¡æœ‰çœŸå®çš„Dragonflyè¿æ¥ï¼Œæˆ‘ä»¬éœ€è¦mock Cookieè·å–
        # ä¸´æ—¶æ›¿æ¢_get_cookieæ–¹æ³•
        original_get_cookie = engine._get_cookie
        async def mock_get_cookie(cid):
            return cookie if cid == cookie_id else None
        engine._get_cookie = mock_get_cookie

        # æ‰§è¡Œä»»åŠ¡
        result = await engine.execute_task_with_streams(task_data)

        # æ¢å¤åŸæ–¹æ³•
        engine._get_cookie = original_get_cookie

        if result.get("success"):
            print(f"âœ… é›ªçƒå¼•æ“æµ‹è¯•æˆåŠŸ!")
            print(f"   ä»»åŠ¡ID: {result.get('task_id')}")
            print(f"   è®°å½•æ•°: {result.get('records_count', 0)}")
            print(f"   å“åº”æ—¶é—´: {result.get('response_time', 0):.2f}ç§’")
            return result
        else:
            print(f"âŒ é›ªçƒå¼•æ“æµ‹è¯•å¤±è´¥: {result.get('error')}")
            return result

    except Exception as e:
        print(f"âŒ é›ªçƒæ ¸å¿ƒå¼•æ“æµ‹è¯•å¼‚å¸¸: {str(e)}")
        return {"success": False, "error": str(e)}


async def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸ§ª Saturn MouseHunter çˆ¬è™«åŠŸèƒ½ç»¼åˆæµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯•è‚¡ç¥¨ä»£ç 
    test_symbol = "SH600000"  # æµ¦å‘é“¶è¡Œ

    # 1. æµ‹è¯•Cookieè·å–
    cookie_id, cookie = await test_cookie_acquisition()
    if not cookie:
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢: æ— æ³•è·å–Cookie")
        return

    # 2. æµ‹è¯•ä»£ç†è·å–
    proxy = await test_proxy_acquisition()

    # 3. æµ‹è¯•ç›´æ¥HTTPæŠ“å–
    direct_result = await test_direct_http_crawling(test_symbol, cookie, proxy)

    # 4. æµ‹è¯•é›ªçƒæ ¸å¿ƒå¼•æ“ï¼ˆå¦‚æœç›´æ¥æŠ“å–æˆåŠŸï¼‰
    if direct_result.get("success"):
        engine_result = await test_xueqiu_core_engine(test_symbol, cookie_id, cookie, proxy)
    else:
        print("\nâš ï¸  è·³è¿‡æ ¸å¿ƒå¼•æ“æµ‹è¯•ï¼ˆç›´æ¥æŠ“å–å¤±è´¥ï¼‰")
        engine_result = None

    # 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   Cookieè·å–: {'âœ… æˆåŠŸ' if cookie else 'âŒ å¤±è´¥'}")
    print(f"   ä»£ç†è·å–: {'âœ… æˆåŠŸ' if proxy else 'âš ï¸  æ— ä»£ç†'}")
    print(f"   ç›´æ¥æŠ“å–: {'âœ… æˆåŠŸ' if direct_result.get('success') else 'âŒ å¤±è´¥'}")
    print(f"   æ ¸å¿ƒå¼•æ“: {'âœ… æˆåŠŸ' if engine_result and engine_result.get('success') else 'âŒ å¤±è´¥' if engine_result else 'âš ï¸  è·³è¿‡'}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    test_report = {
        "test_timestamp": time.time(),
        "test_symbol": test_symbol,
        "cookie_available": bool(cookie),
        "proxy_available": bool(proxy),
        "direct_crawling_result": direct_result,
        "core_engine_result": engine_result,
        "summary": {
            "cookie_test": "success" if cookie else "failed",
            "proxy_test": "success" if proxy else "no_proxy",
            "direct_crawling_test": "success" if direct_result.get("success") else "failed",
            "core_engine_test": "success" if engine_result and engine_result.get("success") else "failed"
        }
    }

    # ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶
    report_file = "/home/cenwei/workspace/saturn_mousehunter/saturn-mousehunter-crawler-service/crawler_test_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(test_report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return test_report


if __name__ == "__main__":
    import time
    asyncio.run(run_comprehensive_test())